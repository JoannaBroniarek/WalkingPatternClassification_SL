---
title: "Project - Statistical Learning"
author: "Francesco, Francisca, Ivana, Joanna, Michele,"
date: "July 5, 2019"
output:
  html_document: default
---
# Introduction

TODO:  Copy from the milestone...

```{r}
# Libraries

suppressMessages(require(randomForest, quietly = T))
suppressMessages(require(caret, quietly = T))
library(MASS)
suppressMessages(require(e1071, quietly = T))



```
-------------------------------------------------------------------

## Data Pre-processing

1. We removed from our data (directly from the Science Journal application) the beginning (1 minute) and the ending (1 minute) as we wanted to ignore time between setting a recording and walking activity.


2. As we are using data that are received from the phone sensors, we are getting rather a noisy data. Hence, the additional filtering is required. In our project we are applying two common techinques: low-pass and high-pass filter. We want to filter out the portion of the acceleration data caused by a gravity from the portion of the data that is caused by motion of the accelometer - the data we are interested in.

  #TODO : to be considered.

**Loading all the csv files as separated dataframes**

```{r}
Ivana.df <- read.csv('Ivana_2walk-Samsung Recording 1-cropped.csv')
Francesco.df <- read.csv('Francesco_2walk-Samsung Recording 2-cropped.csv')
Michele.df <- read.csv('Michele_2walk-Samsung Recording 2-cropped.csv')
Francisca.df <- read.csv('Francisca_2walk-Samsung Recording 1-cropped.csv')
Joanna.df <- read.csv('Asia_2walk-Samsung Recording 1-cropped.csv')
```

**Omitting ALL NULL values in each data frame**

```{r}

### MESSAGE: script works both for data with NANs and without NANs

# Ivana.df <- na.omit(Ivana.df)
# Francesco.df <- na.omit(Francesco.df)
# Michele.df <- na.omit(Michele.df)
# Francisca.df <- na.omit(Francisca.df)
# Joanna.df <- na.omit(Joanna.df)
```

**Data Visualisation**

```{r}
#dataframes <- list(Ivana.df, Francesco.df, Michele.df, Francisca.df, Joanna.df)

acc.type.plots <- function(column="AccX", ylim.min = 0, ylim.max = 28){
  par(mfrow=c(3,2))
  
  plot(Ivana.df$relative_time/1000, Ivana.df[[column]], type = "l", col="red", 
       main = paste0(column, " - Ivana"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
  plot(Francesco.df$relative_time/1000, Francesco.df[[column]], type = "l", col="blue", 
       main = paste0(column, " - Francesco"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
  plot(Michele.df$relative_time/1000, Michele.df[[column]], type = "l", col="green", 
       main = paste0(column, " - Michele"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
  plot(Francisca.df$relative_time/1000, Francisca.df[[column]], type = "l", col="orchid", 
       main = paste0(column, " - Francisca"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
  plot(Joanna.df$relative_time/1000, Joanna.df[[column]], type = "l", col="orange", 
       main = paste0(column, " - Joanna"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
}



acc.type.plots(column="AccX", ylim.min = -3, ylim.max = 30)
acc.type.plots(column="AccY", ylim.min = -11, ylim.max = 15)
acc.type.plots(column="AccZ", ylim.min = -30, ylim.max = 23)
acc.type.plots(column="LinearAccelerometerSensor", ylim.min = -3, ylim.max = 25)
```


-----------------------------


**Slicer** is a function for creating bins for each dataset. 
You need to specify window.size and overlap size for dividing your data.
Function returns a list of all bins i.e. bins = [(1, 2, 3), (3, 4), (5, 6, 7, 8, 9), (10, 11, 12, 13) ...] 
where each number corresponds to a row index of dataframe.

```{r}
### It is a function that obtains possible bins for each dataset

slicer <- function(df, window.size=200, overlap = 100){
  # non-overlap size
  non_overlap <- window.size - overlap
  
  # starting index 
  from <- 1
  
  # starting relative time
  start <- df[["relative_time"]][1]
  
  # vector for created bins
  bins <- list()
  counter <- 1
  
  done <- FALSE
  while (done != TRUE) {
    # get possible bin indexes  = [start, end)
    bin.idx <- c(from, from + window.size)
    
    # get relative time indexes appropriate for that bin and save to bins vector
    bin <- c()
    for(i in start:nrow(df)) ifelse(df$relative_time[i] < bin.idx[2], bin <- c(bin, i), break)
    
    if(is.null(bin) == FALSE) {
      bins[[counter]] <- bin
      counter = counter + 1
    }
    
    # switch starting index for the next bin
    from <- from + non_overlap
    #start <- i
    start <- which(df$relative_time >= from)[1]

    # terminating condition:
    if(from > df[["relative_time"]][nrow(df)]) done <- TRUE
  }
  
  return(bins)
  }
```

Now, we are checking the possible number of bins generated under specific parameters.

```{r, slicer, cache=TRUE}
## Numbers of possible bins with specific window.size and overlap
window.size = 200
overlap = 100

Ivana.bins <- slicer(Ivana.df, window.size=window.size, overlap = overlap)
paste("Ivana", length(Ivana.bins))
Michele.bins <- slicer(Michele.df, window.size=window.size, overlap = overlap)
paste("Michele", length(Michele.bins))
Francesco.bins <- slicer(Francesco.df, window.size=window.size, overlap = overlap)
paste("Francesco",length(Francesco.bins))
Francisca.bins <- slicer(Francisca.df, window.size=window.size, overlap = overlap)
paste("Francisca", length(Francisca.bins))
Joanna.bins <- slicer(Joanna.df, window.size=window.size, overlap = overlap)
paste("Joanna", length(Joanna.bins))
```

This is an example of a first bin for Ivana dataset with window.size=200 and overlap=100.

```{r}
Ivana.df[Ivana.bins[[1]], ]
```


------------------------------
**Generating features**

```{r}
# function which computes the absolute difference
absolute_diff <- function(xx){
  ## calculated only for rows without NANs
  xx <- na.omit(xx)
  return((1/length(xx)) * sum(abs(xx - mean(xx))))
}
```


The **build.features** function takes as an input dataframe and bins for that dataframe and generates a dataframe with features.


```{r}
build.features <- function(df, bins, person_ID){
  # initialize a feature dataframe
  feature.df <- data.frame()
  
  # iterate through bins, generate features and add to feature dataframe
  for(bin.ids in bins){
    bin <- df[bin.ids, ]
    created_obs <- data.frame(mean(bin$AccX, na.rm = TRUE), 
                              sd(bin$AccX, na.rm = TRUE), 
                              absolute_diff(bin$AccX), 
                              mean(bin$AccY, na.rm = TRUE),
                              sd(bin$AccY, na.rm = TRUE),
                              absolute_diff(bin$AccY), 
                              mean(bin$AccZ, na.rm = TRUE), 
                              sd(bin$AccZ, na.rm = TRUE),
                              absolute_diff(bin$AccZ), 
                              mean(bin$LinearAccelerometerSensor, na.rm = TRUE), 
                              person_ID)
    feature.df <- rbind(feature.df, created_obs)
  }
  
  names(feature.df) <- c('meanAccX', 'sdAccX', 'absdiff_AccX', 
                         'meanAccY', 'sdAccY', 'absdiff_AccY',
                         'meanAccZ', 'sdAccZ', 'absdiff_AccZ', 
                         'meanAccLin', 'person_ID')
  return(feature.df)
}

```

Datframe with features + (AccX, AccY, AccZ, AccLin) average behaviour of Francesco's walk.

```{r, features, cache=TRUE}
Francesco.feature.df <- build.features(Francesco.df, Francesco.bins, person_ID = 1)
Francisca.feature.df <- build.features(Francisca.df, Francisca.bins, person_ID = 2)
Ivana.feature.df <- build.features(Ivana.df, Ivana.bins, person_ID = 3)
Joanna.feature.df <- build.features(Joanna.df, Joanna.bins, person_ID = 4)
Michele.feature.df <- build.features(Michele.df, Michele.bins, person_ID = 5)
```


The **plot.features** function is in charge of plotting main four features: AccX, AccY, AccZ and LinAcc.

```{r}
# function that plots main features (AccX, AccY, AccZ, LinAcc)
plot.features <- function(feature.df, person){
  plot(feature.df$meanAccX, col = "blue", ylim = c(-13,18), type = "l",
       ylab = "")
  points(x=1:nrow(feature.df), y = feature.df$meanAccY, 
         type = "l", col = "red")
  points(x=1:nrow(feature.df), y = feature.df$meanAccZ,
         type = "l", col = "green")
  points(x=1:nrow(feature.df), y = feature.df$meanAccLin,
         type = "l", col = "orange")
  title(paste0("Main features of ", person, "`s walking pattern"))
  legend("bottomleft", legend=c("avg(AccX)", 'avg(AccY)', 'avg(AccZ)', 'avg(AccLin)'),
         col = c("blue", 'red', "green", 'orange'), lwd = c(1,1,1,1), bty = "n")
}
```

```{r}
plot.features(Francesco.feature.df, "Francesco")
plot.features(Francisca.feature.df, "Francisca")
plot.features(Ivana.feature.df, "Ivana")
plot.features(Joanna.feature.df, "Joanna")
plot.features(Michele.feature.df, "Michele")
```

-------------------------------------

**Merging all dataframes **

```{r}
walks.df <- rbind(Francesco.feature.df, 
                  Francisca.feature.df,
                  Ivana.feature.df, 
                  Joanna.feature.df, 
                  Michele.feature.df)
# removing NANs
walks.df <- na.omit(walks.df)
# reindex
rownames(walks.df) <- NULL
# show first rows 
head(walks.df)
```

--------------------------------

**Check the correlation**

```{r correlation, cache=TRUE}
suppressMessages(require(corrplot, quietly = T))

cor.df = cor(walks.df[,-1])
corrplot(cor.df, method="number", tl.cex = 0.9, number.cex = 0.5, bg = "gray", addgrid.col = "black")

```
-------------------------------


**The fraction of rows per label**

```{r}
prop.table(table(walks.df$person_ID))
```

---------------------------------------------------

#### **Splitting data**

Splitting dataframe in training and test sets using the **createDataPartition** function. We put 80% of the data to training set.

```{r}
set.seed(13401)

# idx for the training set
idx.train <- caret::createDataPartition(walks.df$person_ID, p = 0.8, list = FALSE)

# creating X_train and y_train sets
X_train <- walks.df[idx.train, ]
y_train <- as.factor(X_train$person_ID)
X_train$person_ID <- NULL

# creating test set
X_test <- walks.df[-idx.train, ]
y_test <- as.factor(X_test$person_ID)
X_test$person_ID <- NULL
```

-----------------------------

## Model elicitation

1. Selected model
2. 10 fold Cross validation
3. predictions
4. Confusion Matrix
5. Evaluation of classifier: Accuracy, Error rate etc.

We are dealing with a multi-class classification. 
Used algorithms:

* Random Forest

* Linear Discriminant Analysis

* Naive Bayes (parametric)

* LASSO Logistic Regression

* k-nearest neighbours   #TODO

* Decision Trees   #TODO

* SVM   #TODO


Proceeding with our super mega giga iper deeep machine Classificat networks neural Bayesian  SKLearning sckit pyplot matplotlib

### Random Forest

```{r, randomForest, cache=TRUE}

randomForest.fit.predict <- function(X_train, y_train, X_test, y_test){ 
  # Model on training data
  model_RF = randomForest(x = X_train, y = y_train, ntree=200, proximity=T)
  
  # Check if all values are predicted (no NA's)
  ifelse(sum(is.na(model_RF$predicted)) != 0, 
         ok <- "There are NAN values in predictions", 
         ok <- "All predictions evaluated")
  print(ok)
  
  # Confusion matrix of the prediction (based on OOB data)
  cat("\nConfusion matrix of the prediction (based on OOB data) : \n")
  print(model_RF$confusion)
  
  ## Error rate based on OOB
  # model_RF$err.rate
  
  # Error on the training set
  err.train <- mean(as.vector(model_RF$predicted) != as.vector(y_train))*100
  cat(sprintf("\nTrain error: %f", round(err.train,2)))
  
  cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Predict from test set
  predictions <- predict(model_RF, newdata = X_test)
  
  # Error on the test set
  err.test <- mean(predictions != y_test)*100
  cat(sprintf("\nTest error: %f \n\n", round(err.test,2)))
  
  statistics <- confusionMatrix(predictions, y_test)
  print(statistics)
  return(predictions)
}  

RF.pred <- randomForest.fit.predict(X_train, y_train, X_test, y_test)
```

### LDA

```{r, lda, cache=TRUE}

lda.fit.predict <- function(X_train, y_train, X_test, y_test){
  # Model
  mod_LDA = lda(y_train ~ ., data = X_train)
  
  # Prediction on train set
  tr_pred_LDA = predict(mod_LDA, X_train)
  
  # Check if all values are predicted (no NA's)
  ifelse(sum(is.na(tr_pred_LDA$class)) != 0, 
         ok <- "There are NAN values in predictions", 
         ok <- "All predictions evaluated")
  print(ok)
  
  # Error on the training set
  ETr_LDA = mean(as.vector(tr_pred_LDA$class) != as.vector(y_train))*100
  cat(sprintf("\nTrain error: %f", round(ETr_LDA,2)))
  
  cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Prediction on test set
  pred_LDA = predict(mod_LDA, X_test)
  
  # Error on the test set
  ETe_LDA <- mean(pred_LDA$class != y_test)*100
  cat(sprintf("\nTest error: %f \n\n", round(ETe_LDA,2)))
  
  # Statistics
  statistics <- confusionMatrix(pred_LDA$class, y_test)
  print(statistics)
  
  return(predictions)
}

lda.pred <- lda.fit.predict(X_train, y_train, X_test, y_test)
```

### Naive Bayes (parametric)

```{r, naiveBayes, cache=TRUE}

naivebayes.fit.predict <- function(X_train, y_train, X_test, y_test){
  # Model
  mod_NB = naiveBayes(y_train ~ ., data = X_train, type = "raw")
  
  # Prediction on train set
  tr_pred = predict(mod_NB, X_train)

  # Check if all values are predicted (no NA's)
  ifelse(sum(is.na(tr_pred)) != 0, 
         ok <- "There are NAN values in predictions", 
         ok <- "All predictions evaluated")
  print(ok)
  
  # Error on the training set
  tr.error = mean(as.vector(tr_pred) != as.vector(y_train))*100
  cat(sprintf("\nTrain error: %f", round(tr.error,2)))
  
  cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Prediction on test set
  pred_NB = predict(mod_NB, X_test)   
  
  # Error on the test set
  MCE_NB = mean(pred_NB !=  y_test)*100
  cat(sprintf("\nTest error: %f \n\n", round(MCE_NB,2)))
  
  # Statistics
  statistics <- confusionMatrix(pred_NB, y_test)
  print(statistics)
  
  return(predictions)
}

naivebayes.pred <- naivebayes.fit.predict(X_train, y_train, X_test, y_test)
```


### LASSO Logistic Regression

```{r, lasso, cache=TRUE}
suppressMessages(require(glmnet, quietly = T)) # l1 penalized-glm 
#suppressMessages(require(doMC, quietly = T))
#registerDoMC( cores = detectCores() )

# Deviance
mod_LLR1 = cv.glmnet(as.matrix(X_train), y_train, family = "multinomial", 
                 type.measure = "deviance",
                 parallel = T)

# Missclassification
mod_LLR2 = cv.glmnet(as.matrix(X_train), y_train, family = "multinomial", 
                 type.measure = "class", 
                 parallel = T)

# Let's take a look at the optimal lambda values (pretty close to each other)
log(c("min" = mod_LLR1$lambda.min , "1se" = mod_LLR1$lambda.1se))
log(c("min" = mod_LLR2$lambda.min, "1se" = mod_LLR2$lambda.1se))

# Now plot (optimal around 5-7 variables!)
#par(mfrow = c(1,2))
#plot(mod_LLR1)
#plot(mod_LLR2)
# I DO NOT HOW TO PROCEED D:
```




 #Additional model: we can choose the best models and build a new classifier based on them. For example: the prediction will be equa to the value that is predicted by majority of them.
