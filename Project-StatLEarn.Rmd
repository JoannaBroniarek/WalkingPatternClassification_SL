---
title: "Project - Statistical Learning"
author: "Francesco, Francisca, Ivana, Joanna, Michele,"
date: "July 5, 2019"
output:
  html_document: default
---
# Introduction

TODO:  Copy from the milestone...

-------------------------------------------------------------------

## Data Pre-processing

1. We removed from our data (directly from the Science Journal application) the beginning (1 minute) and the ending (1 minute) as we wanted to ignore time between setting a recording and walking activity.


2. As we are using data that are received from the phone sensors, we are getting rather a noisy data. Hence, the additional filtering is required. In our project we are applying two common techinques: low-pass and high-pass filter. We want to filter out the portion of the acceleration data caused by a gravity from the portion of the data that is caused by motion of the accelometer - the data we are interested in.

  #TODO : to be considered.

**Loading all the csv files as separated dataframes**

```{r dataframes}
## Libraries
suppressMessages(require(randomForest, quietly = T))
suppressMessages(require(caret, quietly = T))
suppressMessages(require(MASS, quietly = T))
suppressMessages(require(e1071, quietly = T))
suppressMessages(require(doParallel, quietly = T))
suppressMessages(require(corrplot, quietly = T))
suppressMessages(require(glmnet, quietly = T))


Ivana.df <- read.csv('Ivana_2walk-Samsung Recording 1-cropped.csv')
Francesco.df <- read.csv('Francesco_2walk-Samsung Recording 2-cropped.csv')
Michele.df <- read.csv('Michele_2walk-Samsung Recording 2-cropped.csv')
Francisca.df <- read.csv('Francisca_2walk-Samsung Recording 1-cropped.csv')
Joanna.df <- read.csv('Asia_2walk-Samsung Recording 1-cropped.csv')
```

**Omitting ALL NULL values in each data frame**

```{r}

### MESSAGE: script works both for data with NANs and without NANs

# Ivana.df <- na.omit(Ivana.df)
# Francesco.df <- na.omit(Francesco.df)
# Michele.df <- na.omit(Michele.df)
# Francisca.df <- na.omit(Francisca.df)
# Joanna.df <- na.omit(Joanna.df)
```

**Data Visualisation**

```{r}
#dataframes <- list(Ivana.df, Francesco.df, Michele.df, Francisca.df, Joanna.df)

acc.type.plots <- function(column="AccX", ylim.min = 0, ylim.max = 28){
  par(mfrow=c(3,2))
  
  plot(Ivana.df$relative_time/1000, Ivana.df[[column]], type = "l", col="red", 
       main = paste0(column, " - Ivana"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
  plot(Francesco.df$relative_time/1000, Francesco.df[[column]], type = "l", col="blue", 
       main = paste0(column, " - Francesco"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
  plot(Michele.df$relative_time/1000, Michele.df[[column]], type = "l", col="green", 
       main = paste0(column, " - Michele"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
  plot(Francisca.df$relative_time/1000, Francisca.df[[column]], type = "l", col="orchid", 
       main = paste0(column, " - Francisca"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
  plot(Joanna.df$relative_time/1000, Joanna.df[[column]], type = "l", col="orange", 
       main = paste0(column, " - Joanna"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
}



acc.type.plots(column="AccX", ylim.min = -3, ylim.max = 30)
acc.type.plots(column="AccY", ylim.min = -11, ylim.max = 15)
acc.type.plots(column="AccZ", ylim.min = -30, ylim.max = 23)
acc.type.plots(column="LinearAccelerometerSensor", ylim.min = -3, ylim.max = 25)
```


-----------------------------


**Slicer** is a function for creating bins for each dataset. 
You need to specify window.size and overlap size for dividing your data.
Function returns a list of all bins i.e. bins = [(1, 2, 3), (3, 4), (5, 6, 7, 8, 9), (10, 11, 12, 13) ...] 
where each number corresponds to a row index of dataframe.

```{r}
### It is a function that obtains possible bins for each dataset

slicer <- function(df, window.size=200, overlap = 100){
  # non-overlap size
  non_overlap <- window.size - overlap
  
  # starting index 
  from <- 1
  
  # starting relative time
  start <- df[["relative_time"]][1]
  
  # vector for created bins
  bins <- list()
  counter <- 1
  
  done <- FALSE
  while (done != TRUE) {
    # get possible bin indexes  = [start, end)
    bin.idx <- c(from, from + window.size)
    
    # get relative time indexes appropriate for that bin and save to bins vector
    bin <- c()
    for(i in start:nrow(df)) ifelse(df$relative_time[i] < bin.idx[2], bin <- c(bin, i), break)
    
    if(is.null(bin) == FALSE) {
      bins[[counter]] <- bin
      counter = counter + 1
    }
    
    # switch starting index for the next bin
    from <- from + non_overlap
    #start <- i
    start <- which(df$relative_time >= from)[1]

    # terminating condition:
    if(from > df[["relative_time"]][nrow(df)]) done <- TRUE
  }
  
  return(bins)
  }
```

Now, we are checking the possible number of bins generated under specific parameters.

```{r, slicer, cache=TRUE}
## Numbers of possible bins with specific window.size and overlap
window.size = 200
overlap = 100

Ivana.bins <- slicer(Ivana.df, window.size=window.size, overlap = overlap)
paste("Ivana", length(Ivana.bins))
Michele.bins <- slicer(Michele.df, window.size=window.size, overlap = overlap)
paste("Michele", length(Michele.bins))
Francesco.bins <- slicer(Francesco.df, window.size=window.size, overlap = overlap)
paste("Francesco",length(Francesco.bins))
Francisca.bins <- slicer(Francisca.df, window.size=window.size, overlap = overlap)
paste("Francisca", length(Francisca.bins))
Joanna.bins <- slicer(Joanna.df, window.size=window.size, overlap = overlap)
paste("Joanna", length(Joanna.bins))
```

This is an example of a first bin for Ivana dataset with window.size=200 and overlap=100.

```{r}
Ivana.df[Ivana.bins[[1]], ]
```


------------------------------
**Generating features**

```{r}
# function which computes the absolute difference
absolute_diff <- function(xx){
  ## calculated only for rows without NANs
  xx <- na.omit(xx)
  return((1/length(xx)) * sum(abs(xx - mean(xx))))}
```



The **build.features** function takes as an input dataframe and bins for that dataframe and generates a dataframe with features.


```{r}
build.features <- function(df, bins, person_ID){
  # initialize a feature dataframe
  feature.df <- data.frame()
  
  # iterate through bins, generate features and add to feature dataframe
  for(bin.ids in bins){
    bin <- df[bin.ids, ]
    created_obs <- data.frame(mean(bin$AccX, na.rm = TRUE), 
                              sd(bin$AccX, na.rm = TRUE), 
                              absolute_diff(bin$AccX), 
                              mean(bin$AccY, na.rm = TRUE),
                              sd(bin$AccY, na.rm = TRUE),
                              absolute_diff(bin$AccY), 
                              mean(bin$AccZ, na.rm = TRUE), 
                              sd(bin$AccZ, na.rm = TRUE),
                              absolute_diff(bin$AccZ), 
                              mean(bin$LinearAccelerometerSensor, na.rm = TRUE), 
                              person_ID)
    feature.df <- rbind(feature.df, created_obs)
  }
  
  names(feature.df) <- c('meanAccX', 'sdAccX', 'absdiff_AccX', 
                         'meanAccY', 'sdAccY', 'absdiff_AccY',
                         'meanAccZ', 'sdAccZ', 'absdiff_AccZ', 
                         'meanAccLin', 'person_ID')
  return(feature.df)
}
```


Datframe with features + (AccX, AccY, AccZ, AccLin) average behaviour of Francesco's walk.

```{r, features, cache=TRUE}
Francesco.feature.df <- build.features(Francesco.df, Francesco.bins, person_ID = 1)
Francisca.feature.df <- build.features(Francisca.df, Francisca.bins, person_ID = 2)
Ivana.feature.df <- build.features(Ivana.df, Ivana.bins, person_ID = 3)
Joanna.feature.df <- build.features(Joanna.df, Joanna.bins, person_ID = 4)
Michele.feature.df <- build.features(Michele.df, Michele.bins, person_ID = 5)
```


The **plot.features** function is in charge of plotting main four features: AccX, AccY, AccZ and LinAcc.

```{r}
# function that plots main features (AccX, AccY, AccZ, LinAcc)
plot.features <- function(feature.df, person){
  plot(feature.df$meanAccX, col = "blue", ylim = c(-13,18), type = "l",
       ylab = "")
  points(x=1:nrow(feature.df), y = feature.df$meanAccY, 
         type = "l", col = "red")
  points(x=1:nrow(feature.df), y = feature.df$meanAccZ,
         type = "l", col = "green")
  points(x=1:nrow(feature.df), y = feature.df$meanAccLin,
         type = "l", col = "orange")
  title(paste0("Main features of ", person, "`s walking pattern"))
  legend("bottomleft", legend=c("avg(AccX)", 'avg(AccY)', 'avg(AccZ)', 'avg(AccLin)'),
         col = c("blue", 'red', "green", 'orange'), lwd = c(1,1,1,1), bty = "n")
}
```

```{r}
plot.features(Francesco.feature.df, "Francesco")
plot.features(Francisca.feature.df, "Francisca")
plot.features(Ivana.feature.df, "Ivana")
plot.features(Joanna.feature.df, "Joanna")
plot.features(Michele.feature.df, "Michele")
```

-------------------------------------

**Merging all dataframes **

```{r}
walks.df <- rbind(Francesco.feature.df, 
                  Francisca.feature.df,
                  Ivana.feature.df, 
                  Joanna.feature.df, 
                  Michele.feature.df)
# removing NANs
walks.df <- na.omit(walks.df)
# reindex
rownames(walks.df) <- NULL
# show first rows 
head(walks.df)
```

--------------------------------

**Check the correlation**

```{r correlation, cache=TRUE}
cor.df = cor(walks.df[,-1])
corrplot(cor.df, method="number", tl.cex = 0.9, number.cex = 0.5, bg = "gray", addgrid.col = "black")

```
-------------------------------


**Fractions of rows per label**

```{r}
prop.table(table(walks.df$person_ID))
```

---------------------------------------------------

#### **Splitting data**

Splitting dataframe in training and test sets using the **createDataPartition** function. We put 80% of the data to training set.

```{r}
set.seed(13401)

# idx for the training set
idx.train <- caret::createDataPartition(walks.df$person_ID, p = 0.8, list = FALSE)

# creating X_train and y_train sets
X_train <- walks.df[idx.train, ]
y_train <- as.factor(X_train$person_ID)
X_train$person_ID <- NULL

# creating test set
X_test <- walks.df[-idx.train, ]
y_test <- as.factor(X_test$person_ID)
X_test$person_ID <- NULL
```

-----------------------------

## Model elicitation

We are dealing with a multi-class classification. 
Used algorithms:

* Random Forest

* Linear Discriminant Analysis

* QDA   #TODO ?

* Naive Bayes (parametric)

* LASSO Logistic Regression 

* k-Nearest Neighbours

* SVM

* Stacking Model from the best 3 models

```{r}
# function that takes the label according to the highest probability
get_label <- function(predictions){
  n = nrow(predictions)
  pred_labels = rep(NA,n)
  for(i in 1:n){
    max.id <- which.max(predictions[i, ])
    pred_labels[i] <- names(predictions[i, max.id])
  }
  return(as.factor(pred_labels))
}
```

Proceeding with our super mega giga iper deeep machine Classificat networks neural Bayesian  SKLearning sckit pyplot matplotlib

### Random Forest

Choosing the appropriate number of trees (_ntrees_) based on _Out Of Bag_ (prediction) error rates. 

```{r}
# choosing the number of trees - based on X_train
RF.model <- randomForest(x = X_train, y = y_train, ntree=300, proximity=T)

head(RF.model$err.rate)
```

```{r}
# using err.rate values from RF.model we create a dataframe in order to plot 
# the OOB errors using ggplot
oob.error <- data.frame(
  nTrees = rep(1:nrow(RF.model$err.rate), times = 6),
  Error.Type = rep(c('OOB', "1", "2", "3", "4", "5"), each =nrow(RF.model$err.rate)),
  Error = c(RF.model$err.rate[, "OOB"], 
            RF.model$err.rate[, 1],
            RF.model$err.rate[, 2],
            RF.model$err.rate[, 3],
            RF.model$err.rate[, 4],
            RF.model$err.rate[, 5])
)

head(oob.error, 3)
```

```{r}
require(ggplot2)

ggplot(data = oob.error, aes(x=nTrees, y = Error)) + 
  geom_line(aes(color = Error.Type))

```

We observe that the overall and relatives OOB error rates stabilize for a number of trees around 100, an appropriate setting for the parameter could be _ntree = 150_.


```{r, randomForest}
randomForest.fit.predict <- function(X_train, y_train, X_test, y_test, full.stats = TRUE){ 
  cat("\n\n\t\t\t RANDOM FOREST algorithm \n\n")
  # Model on training data
  model_RF = randomForest(x = X_train, y = y_train, ntree=200, proximity=T)
  
  # Check if all values are predicted (no NA's)
  ifelse(sum(is.na(model_RF$predicted)) != 0, 
         ok <- "There are NAN values in predictions", 
         ok <- "All predictions evaluated")
  if(full.stats==TRUE) print(ok)
  
  # Confusion matrix of the prediction (based on OOB data)
  if(full.stats==TRUE) {
  cat("\nConfusion matrix of the prediction (based on OOB data) : \n")
  print(model_RF$confusion)
  }
  ## Error rate based on OOB
  # model_RF$err.rate
  
  # Error on the training set
  err.train <- mean(as.vector(model_RF$predicted) != as.vector(y_train))*100
  cat(sprintf("\nTrain Missclassification rate: %f", round(err.train,2)))
  
  if(full.stats==TRUE) cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Predict from test set
  predictions <- predict(model_RF, newdata = X_test)
  
  # Error on the test set
  err.test <- mean(predictions != y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(err.test,2)))
  
  statistics <- confusionMatrix(predictions, y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
  
  return(predictions)
}  

RF.pred <- randomForest.fit.predict(X_train, y_train, X_test, y_test)
```

### LDA

```{r, lda}

lda.fit.predict <- function(X_train, y_train, X_test, y_test, full.stats=TRUE){
  cat("\n\n\t\t\t LDA - LINEAR DISCRIMINANT ANALYSIS algorithm \n\n")
  # Model
  mod_LDA = lda(y_train ~ ., data = X_train)
  
  # Prediction on train set
  tr_pred_LDA = predict(mod_LDA, X_train)
  
  # Check if all values are predicted (no NA's)
  ifelse(sum(is.na(tr_pred_LDA$class)) != 0, 
         ok <- "There are NAN values in predictions", 
         ok <- "All predictions evaluated")
  if(full.stats==TRUE) print(ok)
  
  # Error on the training set
  ETr_LDA = mean(as.vector(tr_pred_LDA$class) != as.vector(y_train))*100
  cat(sprintf("\nTrain Missclassification rate: %f", round(ETr_LDA,2)))
  
  if(full.stats==TRUE) cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Prediction on test set
  pred_LDA = predict(mod_LDA, X_test)
  
  # Error on the test set
  ETe_LDA <- mean(pred_LDA$class != y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(ETe_LDA,2)))
  
  # Statistics
  statistics <- confusionMatrix(pred_LDA$class, y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
  
  return(pred_LDA)
}

lda.pred <- lda.fit.predict(X_train, y_train, X_test, y_test)
```

### Naive Bayes (parametric)

```{r, naiveBayes}

naivebayes.fit.predict <- function(X_train, y_train, X_test, y_test, full.stats=TRUE){
  cat("\n\n\t\t\t NAIVE BAYES algorithm \n\n")
  
  # Model
  mod_NB = naiveBayes(y_train ~ ., data = X_train, type = "raw")
  
  # Prediction on train set
  tr_pred = predict(mod_NB, X_train)

  # Check if all values are predicted (no NA's)
  ifelse(sum(is.na(tr_pred)) != 0, 
         ok <- "There are NAN values in predictions", 
         ok <- "All predictions evaluated")
  if(full.stats==TRUE) print(ok)
  
  # Error on the training set
  tr.error = mean(as.vector(tr_pred) != as.vector(y_train))*100
  cat(sprintf("\nTrain Missclassification rate: %f", round(tr.error,2)))
  
  if(full.stats==TRUE) cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Prediction on test set
  pred_NB = predict(mod_NB, X_test)   
  
  # Error on the test set
  MCE_NB = mean(pred_NB !=  y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(MCE_NB,2)))
  
  # Statistics
  statistics <- confusionMatrix(pred_NB, y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
  
  return(pred_NB)
}

naivebayes.pred <- naivebayes.fit.predict(X_train, y_train, X_test, y_test)
```

### k-Nearest Neighbor

Cross validation in order to get the best k.

```{r knnCV}
set.seed(123)
registerDoParallel(cores=1)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 2)

knn_fit <- train(y = y_train ,
                 x = X_train, method = "knn",
                 trControl  = trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 10)
# Take a look
print(paste("Best parameter:", as.character(knn_fit$bestTune)))
plot(knn_fit)
```

Using Cross Validation method as the best parameter we received k=7.

```{r knnPred}
knn.fit.predict <- function(X_train, y_train, X_test, y_test, k=knn_fit$bestTune, full.stats=TRUE){
  cat("\n\n\t\t\t k-NEAREST NEIGHBOR algorithm \n\n")
  
  # Model
  mod_KNN = knn3(y=y_train, x = X_train, k)
  
  # Prediction on train set
  tr_pred = predict(mod_KNN, X_train)
  main_tr_pred = get_label(tr_pred)

  # Check if all values are predicted (no NA's)
  ifelse(sum(is.na(main_tr_pred)) != 0, 
         ok <- "There are NAN values in predictions", 
         ok <- "All predictions evaluated")
  if(full.stats==TRUE) print(ok)
  
  # Error on the training set
  tr.error = mean(as.vector(main_tr_pred) != as.vector(y_train))*100
  cat(sprintf("\nTrain Missclassification rate: %f", round(tr.error,2)))
  
  if(full.stats==TRUE) cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Prediction on test set
  pred_KNN = predict(mod_KNN, X_test)
  main_pred_KNN = get_label(pred_KNN)

  # Error on the test set
  te.error = mean(main_pred_KNN !=  y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(te.error,2)))

  # Statistics
  statistics <- confusionMatrix(main_pred_KNN, y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
    
  return(main_pred_KNN)
}

k_best = 7
knn.pred <- knn.fit.predict(X_train, y_train, X_test, y_test, k = k_best)
```


### LASSO Logistic Regression

```{r, lasso}

lasso.fit.predict <- function(X_train, y_train, X_test, y_test, full.stats=TRUE){
  cat("\n\n\t\t\t LASSO algorithm \n\n")
  
  mod_LASSO = cv.glmnet(as.matrix(X_train), y_train, 
                        family = "multinomial", 
                        type.measure = "class",
                        parallel = T)
  if(full.stats==TRUE) print(c("min" = mod_LASSO$lambda.min, "1se" = mod_LASSO$lambda.1se))
  
  pred_tr_LASSO = predict(mod_LASSO, newx = as.matrix(X_train), 
                          type = "class", 
                          s = mod_LASSO$lambda.1se)

  # Error on the training set
  tr.error = mean(as.vector(pred_tr_LASSO) != as.vector(y_train))*100
  cat(sprintf("\nTrain Missclassification rate: %f", round(tr.error,2)))
  
  if(full.stats==TRUE) cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Prediction on test set
  pred_LASSO = predict(mod_LASSO, newx = as.matrix(X_test), type = "class", s = mod_LASSO$lambda.1se)
  test_pred_LASSO = as.factor(pred_LASSO)

  # Error on the test set
  te.error = mean(test_pred_LASSO !=  y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(te.error,2)))

  # Statistics
  statistics <- confusionMatrix(test_pred_LASSO, y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
  
  return(test_pred_LASSO)
}

lasso.pred <-lasso.fit.predict(X_train, y_train, X_test, y_test)
```

### SVM

```{r svm}

svm.fit.predict <- function(X_train, y_train, X_test, y_test, full.stats=TRUE){
  cat("\n\n\t\t\t SVM algorithm \n\n")
  
  # Model
  mod.svm <-svm(x = X_train, y = y_train)
  
  # Prediction on train set
  train.pred <- predict(mod.svm, X_train)

  # Check if all values are predicted (no NA's)
  ifelse(sum(is.na(train.pred)) != 0, 
         ok <- "There are NAN values in predictions", 
         ok <- "All predictions evaluated")
  if(full.stats==TRUE) print(ok)
  
  # Error on the training set
  tr.error = mean(as.vector(train.pred) != as.vector(y_train))*100
  cat(sprintf("\nTrain Missclassification rate: %f", round(tr.error,2)))
  
  if(full.stats==TRUE) cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Prediction on test set
  pred_svm = predict(mod.svm, X_test)

  # Error on the test set
  te.error = mean(pred_svm !=  y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(te.error,2)))

  # Statistics
  statistics <- confusionMatrix(pred_svm, y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
    
  return(pred_svm)
}

svm.pred <- svm.fit.predict(X_train, y_train, X_test, y_test)
```



#### Summary of algorithms results:

rand.forest.acuracy <- 0.9108 
lda.accuracy <- 0.7164 
naive.bayes.accuracy <- 0.6099
kn.accuracy <- 0.919
lasso.accuracy <- 0.7266
svm.accuracy <- 0.88

|          | RndForest | LDA    | NaiveBayes | LASSO  | k-NN   | SVM  | Stacking |
|----------|-----------|--------|------------|--------|--------|------|----------|
| Accuracy | 0.9114    | 0.7164 | 0.6099     | 0.7266 | 0.7266 | 0.88 | 0.9184   |

#### Stacking Model

```{r}
## Majority voting using all the models

stacking.pred.all <- cbind(knn   = as.character(knn.pred),
                           RF    = as.character(RF.pred),
                           LDA   = as.character(lda.pred$class),
                           NaivB = as.character(naivebayes.pred),
                           Lasso = as.character(lasso.pred),
                           SVM   = as.character(svm.pred)    )

# finding for each row (obs) the mode
stacking.pred.all <- as.factor( apply(stacking.pred.all, 1, function(x) names(which.max(table(x))) ) )
                  
confusionMatrix(stacking.pred.all, y_test)
```
We built **the stacking model** using the best tree models: k-NN, Random Forest and SVM. 
The idea is to use the predictions of majority. In a case that each classifier returns different class, we follow using the prediction of the best classifier: k-NN. 

```{r stacking}
stacking.model <- function(m1, m2, m3){
  # m1, m2, m3 - are models with a high performance, whereas m1 - is the best model
  
  # function to get a mode
  getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
  }
  # finding predictions from stacking model
  n <- length(m1)
  final.pred <- rep(NA, n)
  # loop through predictions
  for(i in 1:n){
    if(m1[i]!=m2[i] & m2[i]!=m3[i] & m1[i]!=m3[i]) {final.pred[i] = m1[i]}
    else {final.pred[i] = getmode(c(m1[i], m2[i], m3[i]))}
  }
  return(final.pred)
}

#stacking.pred <- stacking.model(knn.pred = knn.pred, random.forest.pred = RF.pred, svm.pred = svm.pred)
stacking.pred <- stacking.model( knn.pred, RF.pred,  svm.pred)
```


```{r stacking2}
stacking.statistics <- function(stacking.pred, y_test, full.stats=TRUE){
  cat("\n\n\t\t\t STACKING MODEL \n\n")
  
  te.error = mean(stacking.pred !=  y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(te.error,2)))

  # Statistics
  statistics <- confusionMatrix(as.factor(stacking.pred), y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
}

stacking.statistics(stacking.pred, y_test)
```



-----------------------------------------------

## Analysis for different bin sizes

## Numbers of possible bins with specific window.size and overlap

```{r pipeline}
whole.pipeline <- function(Ivana.df, Michele.df, Francesco.df, Francisca.df, Joanna.df, window.size, overlap){
  cat(paste("#################\n","window.size  = ", window.size, "\noverlap = ", overlap, "\n#################"))
  
  ## Dividing into bins
  Ivana.bins <- slicer(Ivana.df, window.size=window.size, overlap = overlap)
  cat(paste("\nIvana", length(Ivana.bins)))
  Michele.bins <- slicer(Michele.df, window.size=window.size, overlap = overlap)
  cat(paste("\nMichele", length(Michele.bins)))
  Francesco.bins <- slicer(Francesco.df, window.size=window.size, overlap = overlap)
  cat(paste("\nFrancesco",length(Francesco.bins)))
  Francisca.bins <- slicer(Francisca.df, window.size=window.size, overlap = overlap)
  cat(paste("\nFrancisca", length(Francisca.bins)))
  Joanna.bins <- slicer(Joanna.df, window.size=window.size, overlap = overlap)
  cat(paste("\nJoanna", length(Joanna.bins)))
  
  ## Building feature dataframes
  Francesco.feature.df <- build.features(Francesco.df, Francesco.bins, person_ID = 1)
  Francisca.feature.df <- build.features(Francisca.df, Francisca.bins, person_ID = 2)
  Ivana.feature.df <- build.features(Ivana.df, Ivana.bins, person_ID = 3)
  Joanna.feature.df <- build.features(Joanna.df, Joanna.bins, person_ID = 4)
  Michele.feature.df <- build.features(Michele.df, Michele.bins, person_ID = 5)
  
  ## Merging into one dataframe
  walks.df <- rbind(Francesco.feature.df, 
                  Francisca.feature.df,
                  Ivana.feature.df, 
                  Joanna.feature.df, 
                  Michele.feature.df)
  walks.df <- na.omit(walks.df)
  rownames(walks.df) <- NULL # reindex
  
  ########     MACHINE LEARNING
  
  ## Splitting into Test Set and Train Set
  idx.train <- caret::createDataPartition(walks.df$person_ID, p = 0.8, list = FALSE)
  X_train <- walks.df[idx.train, ]
  y_train <- as.factor(X_train$person_ID)
  X_train$person_ID <- NULL

  X_test <- walks.df[-idx.train, ]
  y_test <- as.factor(X_test$person_ID)
  X_test$person_ID <- NULL
  
  ## Models and Predictions
  RF.pred <- randomForest.fit.predict(X_train, y_train, X_test, y_test, full.stats = FALSE)
  lda.pred <- lda.fit.predict(X_train, y_train, X_test, y_test, full.stats = FALSE)
  naivebayes.pred <- naivebayes.fit.predict(X_train, y_train, X_test, y_test, full.stats = FALSE)
  lasso.pred <-lasso.fit.predict(X_train, y_train, X_test, y_test, full.stats = FALSE)
  knn.pred <- knn.fit.predict(X_train, y_train, X_test, y_test, k=7, full.stats = FALSE) #TODO: k via CrossVal
  svm.pred <- svm.fit.predict(X_train, y_train, X_test, y_test, full.stats = FALSE)

  # Stacking model
  stacking.pred <- stacking.model(m1 = knn.pred, m2 = RF.pred, m3 = svm.pred)
  stacking.statistics(stacking.pred, y_test, full.stats = FALSE)
}

whole.pipeline(Ivana.df, Michele.df, Francesco.df, Francisca.df, Joanna.df, window.size=500, overlap=200)
```
